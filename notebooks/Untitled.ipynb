{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4c66cce-7b14-4ee7-9785-82ce5e9e4fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7cdcbba-f5da-49c6-abf6-afa5d1426318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "507efdce-0b71-4819-986c-7b50c96754c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pl.read_csv(data_dir / \"01_raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "80869d48-ed14-4fd0-b3aa-5c3b08b3629f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oil = pl.scan_csv(data_dir / \"01_raw/oil.csv\").select([\n",
    "pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\"),\n",
    "    pl.col(\"dcoilwtico\").alias(\"price\"),\n",
    "]).filter(pl.col(\"price\").is_not_null()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "66e7a93f-d14e-4509-ae31-a5a42baea963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_to_scale = [\"price\"]\n",
    "oil_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "oil_scaled = oil.with_columns(\n",
    "    pl.DataFrame(oil_scaler.fit_transform(\n",
    "        oil.select(pl.col(cols_to_scale)).to_numpy()\n",
    "    ), schema=cols_to_scale).select(pl.all().suffix(\"_scaled\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2a930a49-793d-439f-81a6-4e5714373645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oil_arr = oil_scaled.select(\"price_scaled\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "83111a81-3e0c-40da-926a-d055f806ef76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "43d3dc2b-5b19-4a6f-8d64-54b6f55964f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train, y_val = train_test_split(oil_arr, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7eba1c85-6eb7-4e0a-a721-e5b8c0465ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2bdc217e-7717-43b4-8452-f924059e84fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b13ae20e-4e6c-457a-ae8d-192babc25993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OilPricesDataset(Dataset):\n",
    "    def __init__(self, array, batch_length):\n",
    "        self.array = torch.from_numpy(array).type(torch.float32)\n",
    "        self.batch_length = batch_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.array.shape[0] - self.batch_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_arr = idx + self.batch_length\n",
    "        return self.array[id_arr - self.batch_length: id_arr], self.array[id_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b5980fc4-5ed7-4294-8976-21c4df43c204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = 5\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(OilPricesDataset(y_train, train_size), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(OilPricesDataset(y_val, train_size), batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d2546e47-0df4-4c67-8e53-ce2f767aeaac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(train_size * batch_size, batch_size),)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "5c2eb3d6-1928-4a86-bca0-41de2fbb53b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_features, n_hidden, n_outputs, sequence_len, n_lstm_layers=1, n_deep_layers=10, use_cuda=False, dropout=0.2):\n",
    "        '''\n",
    "        n_features: number of input features (1 for univariate forecasting)\n",
    "        n_hidden: number of neurons in each hidden layer\n",
    "        n_outputs: number of outputs to predict for each training example\n",
    "        n_deep_layers: number of hidden dense layers after the lstm layer\n",
    "        sequence_len: number of steps to look back at for prediction\n",
    "        dropout: float (0 < dropout < 1) dropout ratio between dense layers\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "        self.nhid = n_hidden\n",
    "        self.use_cuda = use_cuda # set option for device selection\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(n_features,\n",
    "                            n_hidden,\n",
    "                            num_layers=n_lstm_layers,\n",
    "                            batch_first=True) # As we have transformed our data in this way\n",
    "\n",
    "        # first dense after lstm\n",
    "        self.fc1 = nn.Linear(n_hidden * sequence_len, n_hidden) \n",
    "        # Dropout layer \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create fully connected layers (n_hidden x n_deep_layers)\n",
    "        dnn_layers = []\n",
    "        for i in range(n_deep_layers):\n",
    "          # Last layer (n_hidden x n_outputs)\n",
    "          if i == n_deep_layers - 1:\n",
    "            dnn_layers.append(nn.ReLU())\n",
    "            dnn_layers.append(nn.Linear(nhid, n_outputs))\n",
    "          # All other layers (n_hidden x n_hidden) with dropout option\n",
    "          else:\n",
    "            dnn_layers.append(nn.ReLU())\n",
    "            dnn_layers.append(nn.Linear(nhid, nhid))\n",
    "            if dropout:\n",
    "                dnn_layers.append(nn.Dropout(p=dropout))\n",
    "        # compile DNN layers\n",
    "        self.dnn = nn.Sequential(*dnn_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden_state = torch.zeros(self.n_lstm_layers, x.shape[0], self.nhid)\n",
    "        cell_state = torch.zeros(self.n_lstm_layers, x.shape[0], self.nhid)\n",
    "\n",
    "        # move hidden state to device\n",
    "        if self.use_cuda:\n",
    "            hidden_state = hidden_state.to(device)\n",
    "            cell_state = cell_state.to(device)\n",
    "\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "\n",
    "        # Forward Pass\n",
    "        x, h = self.lstm(x, self.hidden) # LSTM\n",
    "        x = self.dropout(x.contiguous().view(x.shape[0], -1)) # Flatten lstm out \n",
    "        x = self.fc1(x) # First Dense\n",
    "        return self.dnn(x) # Pass forward through fully connected DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "6d33fae0-8969-4ff9-9b48-850f32b307fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nhid = 50 # Number of nodes in the hidden layer\n",
    "n_dnn_layers = 5 # Number of hidden fully connected layers\n",
    "nout = 1 # Prediction Window\n",
    "sequence_len = train_size # Training Window\n",
    "\n",
    "# Number of features (since this is a univariate timeseries we'll set\n",
    "# this to 1 -- multivariate analysis is coming in the future)\n",
    "ninp = 1\n",
    "\n",
    "# Device selection (CPU | GPU)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = 'cuda' if USE_CUDA else 'cpu'\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMForecaster(ninp, nhid, nout, sequence_len, n_deep_layers=n_dnn_layers, use_cuda=USE_CUDA).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "06783c7b-2417-452d-8a25-fc0d8bd7662d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set learning rate and number of epochs to train over\n",
    "lr = 4e-4\n",
    "n_epochs = 20\n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "226f47e5-0121-4b12-8485-4cf966c3eda3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - train: 0.9996164298575857, valid: 1.0098348639228127\n",
      "1 - train: 1.0000891465207804, valid: 0.9994138533418829\n",
      "2 - train: 0.9988052559935529, valid: 1.0242966196753762\n",
      "3 - train: 0.9935230081496031, valid: 1.0299051187255166\n",
      "4 - train: 0.9952625917351764, valid: 1.0124791643836282\n",
      "5 - train: 0.999161668445753, valid: 1.0083116238767451\n",
      "6 - train: 0.9938781896363134, valid: 1.0196453983133489\n",
      "7 - train: 0.997894841691722, valid: 1.0191043940457432\n",
      "8 - train: 0.9997080098027769, valid: 1.025453887202523\n",
      "9 - train: 0.999415617922078, valid: 1.0203527754003352\n",
      "10 - train: 0.9967144131660461, valid: 1.014822244644165\n",
      "11 - train: 0.9990392508714095, valid: 1.0340031656351956\n",
      "12 - train: 0.9929921588172084, valid: 1.0236997116695752\n",
      "13 - train: 0.9922063467295273, valid: 1.0434598976915532\n",
      "14 - train: 0.986374477977338, valid: 1.0312170819802717\n",
      "15 - train: 0.9958538127982098, valid: 1.0256436196240513\n",
      "16 - train: 0.9890517566515051, valid: 1.0354358174584128\n",
      "17 - train: 0.9917583323043325, valid: 1.024771267717535\n",
      "18 - train: 0.9846586572087329, valid: 1.0446557619354941\n",
      "19 - train: 0.9879433564517809, valid: 1.0314539887688376\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[321], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m     v_losses\u001b[38;5;241m.\u001b[39mappend(valid_loss)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, valid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mplot_losses\u001b[49m(t_losses, v_losses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_losses' is not defined"
     ]
    }
   ],
   "source": [
    "# Lists to store training and validation losses\n",
    "t_losses, v_losses = [], []\n",
    "# Loop over epochs\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "\n",
    "    # train step\n",
    "    model.train()\n",
    "    # Loop over train dataset\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # move inputs to device\n",
    "        x = x.to(device)\n",
    "        y  = y.squeeze().to(device)\n",
    "        # Forward Pass\n",
    "        preds = model(x).squeeze()\n",
    "        loss = criterion(preds, y) # compute batch loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = train_loss / len(train_loader)\n",
    "    t_losses.append(epoch_loss)\n",
    "\n",
    "    # validation step\n",
    "    model.eval()\n",
    "    # Loop over validation dataset\n",
    "    for x, y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            x, y = x.to(device), y.squeeze().to(device)\n",
    "            preds = model(x).squeeze()\n",
    "            error = criterion(preds, y)\n",
    "        valid_loss += error.item()\n",
    "    valid_loss = valid_loss / len(val_loader)\n",
    "    v_losses.append(valid_loss)\n",
    "\n",
    "    print(f'{epoch} - train: {epoch_loss}, valid: {valid_loss}')\n",
    "\n",
    "\n",
    "plot_losses(t_losses, v_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823a2d7-73b3-4219-b271-d9e14c3d93c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e710099b-63ce-4cdb-be97-f1efc88db3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaba172-4a74-4c8d-8c22-e41f4d50452f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a0f0ba9b-96f3-4643-a9e6-c294f831f18f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.831569\n",
      "Epoch: 1, Loss: 0.379933\n",
      "Epoch: 2, Loss: 0.375677\n",
      "Epoch: 3, Loss: 0.174501\n",
      "Epoch: 4, Loss: 0.105528\n",
      "Epoch: 5, Loss: 0.044817\n",
      "Epoch: 6, Loss: 0.061509\n",
      "Epoch: 7, Loss: 0.027500\n",
      "Epoch: 8, Loss: 0.021195\n",
      "Epoch: 9, Loss: 0.008202\n",
      "Epoch: 10, Loss: 0.006289\n",
      "Epoch: 11, Loss: 0.007742\n",
      "Epoch: 12, Loss: 0.010367\n",
      "Epoch: 13, Loss: 0.006189\n",
      "Epoch: 14, Loss: 0.005130\n",
      "Epoch: 15, Loss: 0.011133\n",
      "Epoch: 16, Loss: 0.006254\n",
      "Epoch: 17, Loss: 0.007326\n",
      "Epoch: 18, Loss: 0.003874\n",
      "Epoch: 19, Loss: 0.004246\n",
      "Epoch: 20, Loss: 0.005532\n",
      "Epoch: 21, Loss: 0.007784\n",
      "Epoch: 22, Loss: 0.007530\n",
      "Epoch: 23, Loss: 0.002169\n",
      "Epoch: 24, Loss: 0.008489\n",
      "Epoch: 25, Loss: 0.005591\n",
      "Epoch: 26, Loss: 0.005705\n",
      "Epoch: 27, Loss: 0.009285\n",
      "Epoch: 28, Loss: 0.004243\n",
      "Epoch: 29, Loss: 0.007454\n",
      "Epoch: 30, Loss: 0.002393\n",
      "Epoch: 31, Loss: 0.002804\n",
      "Epoch: 32, Loss: 0.008110\n",
      "Epoch: 33, Loss: 0.003389\n",
      "Epoch: 34, Loss: 0.010065\n",
      "Epoch: 35, Loss: 0.004284\n",
      "Epoch: 36, Loss: 0.010099\n",
      "Epoch: 37, Loss: 0.005928\n",
      "Epoch: 38, Loss: 0.007152\n",
      "Epoch: 39, Loss: 0.004375\n",
      "Epoch: 40, Loss: 0.004882\n",
      "Epoch: 41, Loss: 0.006940\n",
      "Epoch: 42, Loss: 0.006054\n",
      "Epoch: 43, Loss: 0.008194\n",
      "Epoch: 44, Loss: 0.005711\n",
      "Epoch: 45, Loss: 0.003541\n",
      "Epoch: 46, Loss: 0.006202\n",
      "Epoch: 47, Loss: 0.005117\n",
      "Epoch: 48, Loss: 0.002597\n",
      "Epoch: 49, Loss: 0.005206\n",
      "Epoch: 50, Loss: 0.004221\n",
      "Epoch: 51, Loss: 0.006202\n",
      "Epoch: 52, Loss: 0.003235\n",
      "Epoch: 53, Loss: 0.004167\n",
      "Epoch: 54, Loss: 0.010787\n",
      "Epoch: 55, Loss: 0.004481\n",
      "Epoch: 56, Loss: 0.006772\n",
      "Epoch: 57, Loss: 0.005451\n",
      "Epoch: 58, Loss: 0.005404\n",
      "Epoch: 59, Loss: 0.004955\n",
      "Epoch: 60, Loss: 0.005831\n",
      "Epoch: 61, Loss: 0.008626\n",
      "Epoch: 62, Loss: 0.004254\n",
      "Epoch: 63, Loss: 0.003803\n",
      "Epoch: 64, Loss: 0.007025\n",
      "Epoch: 65, Loss: 0.006225\n",
      "Epoch: 66, Loss: 0.006328\n",
      "Epoch: 67, Loss: 0.004381\n",
      "Epoch: 68, Loss: 0.004822\n",
      "Epoch: 69, Loss: 0.004665\n",
      "Epoch: 70, Loss: 0.005239\n",
      "Epoch: 71, Loss: 0.002983\n",
      "Epoch: 72, Loss: 0.013269\n",
      "Epoch: 73, Loss: 0.004012\n",
      "Epoch: 74, Loss: 0.003212\n",
      "Epoch: 75, Loss: 0.007982\n",
      "Epoch: 76, Loss: 0.003615\n",
      "Epoch: 77, Loss: 0.011258\n",
      "Epoch: 78, Loss: 0.005900\n",
      "Epoch: 79, Loss: 0.004831\n",
      "Epoch: 80, Loss: 0.008175\n",
      "Epoch: 81, Loss: 0.005209\n",
      "Epoch: 82, Loss: 0.005524\n",
      "Epoch: 83, Loss: 0.004422\n",
      "Epoch: 84, Loss: 0.007782\n",
      "Epoch: 85, Loss: 0.004785\n",
      "Epoch: 86, Loss: 0.004333\n",
      "Epoch: 87, Loss: 0.002348\n",
      "Epoch: 88, Loss: 0.007242\n",
      "Epoch: 89, Loss: 0.004135\n",
      "Epoch: 90, Loss: 0.006211\n",
      "Epoch: 91, Loss: 0.004196\n",
      "Epoch: 92, Loss: 0.003133\n",
      "Epoch: 93, Loss: 0.005648\n",
      "Epoch: 94, Loss: 0.006036\n",
      "Epoch: 95, Loss: 0.007493\n",
      "Epoch: 96, Loss: 0.005150\n",
      "Epoch: 97, Loss: 0.003630\n",
      "Epoch: 98, Loss: 0.003321\n",
      "Epoch: 99, Loss: 0.006078\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for img, label in oil_dl:\n",
    "        out = model(img.view(-1).squeeze())\n",
    "        loss = loss_fn(out, label.view(-1))\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a3a134e7-f97c-4d80-a0fc-7e492f1c999a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "view(): argument 'size' must be tuple of ints, but found element of type ellipsis at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[304], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m oil_scaler\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mTypeError\u001b[0m: view(): argument 'size' must be tuple of ints, but found element of type ellipsis at pos 2"
     ]
    }
   ],
   "source": [
    "oil_scaler.inverse_transform(out.view(1,...).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92248c-aec4-4d0b-b8c7-6e0ecff6327e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "store-forecasting py310",
   "language": "python",
   "name": "store-fcst-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
